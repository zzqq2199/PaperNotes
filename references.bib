
@article{checkmate,
  title={Checkmate: Breaking the memory wall with optimal tensor rematerialization},
  author={Jain, Paras and Jain, Ajay and Nrusimha, Aniruddha and Gholami, Amir and Abbeel, Pieter and Gonzalez, Joseph and Keutzer, Kurt and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={497--511},
  year={2020}
}
@article{tang2022delta,
  title={DELTA: Dynamically Optimizing GPU Memory beyond Tensor Recomputation},
  author={Tang, Yu and Wang, Chenyu and Zhang, Yufan and Liu, Yuliang and Zhang, Xingcheng and Qiao, Linbo and Lai, Zhiquan and Li, Dongsheng},
  journal={arXiv preprint arXiv:2203.15980},
  year={2022}
}

@article{megatron3,
  title={Reducing Activation Recomputation in Large Transformer Models},
  author={Korthikanti, Vijay and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2205.05198},
  year={2022}
}


@inproceedings{courbariaux2015binaryconnect,
  title={Binaryconnect: Training deep neural networks with binary weights during propagations},
  author={Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  booktitle={Advances in neural information processing systems},
  pages={3123--3131},
  year={2015}
}
@article{gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
 
@inproceedings{rhu2016vdnn,
  title={vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design},
  author={Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={1--13},
  year={2016},
  organization={IEEE}
}
@inproceedings{zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}
@article{zero-offload,
  title={Zero-offload: Democratizing billion-scale model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  journal={arXiv preprint arXiv:2101.06840},
  year={2021}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}
@inproceedings{wang2018superneurons,
  title={Superneurons: Dynamic GPU memory management for training deep neural networks},
  author={Wang, Linnan and Ye, Jinmian and Zhao, Yiyang and Wu, Wei and Li, Ang and Song, Shuaiwen Leon and Xu, Zenglin and Kraska, Tim},
  booktitle={Proceedings of the 23rd ACM SIGPLAN symposium on principles and practice of parallel programming},
  pages={41--53},
  year={2018}
}
@inproceedings{kim2019parallax,
  title={Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks},
  author={Kim, Soojeong and Yu, Gyeong-In and Park, Hojin and Cho, Sungwoo and Jeong, Eunji and Ha, Hyeonmin and Lee, Sanha and Jeong, Joo Seong and Chun, Byung-Gon},
  booktitle={Proceedings of the Fourteenth EuroSys Conference 2019},
  pages={43},
  year={2019},
  organization={ACM}
}

%GradDrop
@article{aji2017sparse,
  title={Sparse communication for distributed gradient descent},
  author={Aji, Alham Fikri and Heafield, Kenneth},
  journal={arXiv preprint arXiv:1704.05021},
  year={2017}
}
%DGC
@article{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  journal={arXiv preprint arXiv:1712.01887},
  year={2017}
}

% TBQ
@inproceedings{strom2015scalable,
 title={Scalable distributed DNN training using commodity GPU cloud computing},
  author={Strom, Nikko},
  booktitle={Proceedings of Sixteenth Annual Conference of the International Speech Communication Association},
 year={2015}
}

%TernGrad
@inproceedings{wen2017terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Proceedings of Advances in neural information processing systems},
  pages={1509--1519},
  year={2017}
}

%ECQ-SGD
@article{wu2018error,
  title={Error compensated quantized SGD and its applications to large-scale distributed optimization},
  author={Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
  journal={arXiv preprint arXiv:1806.08054},
  year={2018}
}

@inproceedings{peng2019bytescheduler,
  title={A generic communication scheduler for distributed DNN training acceleration},
  author={Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles (ACM SOSP 2019), Huntsville, Ontario, Canada, October 27-30, 2019},
  year={2019}
}

@article{hashemi2018tictac,
  title={TicTac: Accelerating distributed deep learning with communication scheduling},
  author={Hashemi, Sayed Hadi and Jyothi, Sangeetha Abdu and Campbell, Roy H},
  journal={arXiv preprint arXiv:1803.03288},
  year={2018}
}

@article{jayarajan2019priority,
  title={Priority-based parameter propagation for distributed DNN training},
  author={Jayarajan, Anand and Wei, Jinliang and Gibson, Garth and Fedorova, Alexandra and Pekhimenko, Gennady},
  journal={arXiv preprint arXiv:1905.03960},
  year={2019}
}

@article{horovod,
  author    = {Alexander Sergeev and
               Mike Del Balso},
  title     = {Horovod: fast and easy distributed deep learning in TensorFlow},
  journal   = {CoRR},
  volume    = {abs/1802.05799},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05799},
  archivePrefix = {arXiv},
  eprint    = {1802.05799},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-05799},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{chen2015mxnet,
  title={MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal={arXiv preprint arXiv:1512.01274},
  year={2015}
}

@inproceedings{abadi2016tensorflow,
  title={TensorFlow: A System for Large-Scale Machine Learning.},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={Proceedings of OSDI},
  volume={16},
  pages={265--283},
  year={2016}
}

@inproceedings{li2014scaling,
  title={Scaling Distributed Machine Learning with the Parameter Server.},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={Proceedings of OSDI},
  volume={14},
  pages={583--598},
  year={2014}
}


@Misc{paddlepaddlecode,
title = {{PaddlePaddle GitHub Source Code}},
howpublished = {\url{https://github.com/PaddlePaddle/Paddle}},
note = "[Online; accessed July-2022]",
}

@Misc{open-source-Terngrad,
title = {{TernGrad implemented in PyTorch}},
howpublished = {\url{https://github.com/pytorch/pytorch/blob/master/caffe2/operators/fused_rowwise_random_quantization_ops.cc}},
note = "[Online; accessed July-2022]",
}


@Misc{bank-conflicts-in-GPU,
title = {{Bank conflict in GPU}},
howpublished=
{\url{https://devblogs.nvidia.com/using-shared-memory-cuda-cc/}},
note="[Online; accessed July-2022]",
}

@Misc{open-source-TBQ,
title = {{Two-bit Quantization in MXNet}},
howpublished = {\url{https://github.com/apache/incubator-mxnet/blob/master/src/kvstore/gradient_compression-inl.h}},
note = "[Online; accessed July-2022]",
}

@Misc{bank-conflict,
title = {{bank conflict in cuda}},
howpublished = {\url{https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#maximize-memory-throughput}},
note = "[Online; accessed July-2022]",
}

@inproceedings{Peng2018Optimus,
 author = {Peng, Yanghua and Bao, Yixin and Chen, Yangrui and Wu, Chuan and Guo, Chuanxiong},
 title = {Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters},
 booktitle = {Proceedings of the Thirteenth EuroSys Conference},
 series = {EuroSys '18},
 year = {2018},
 isbn = {978-1-4503-5584-1},
 location = {Porto, Portugal},
 pages = {3:1--3:14},
 articleno = {3},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3190508.3190517},
 doi = {10.1145/3190508.3190517},
 acmid = {3190517},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deep learning, resource management},
} 

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

%data parallelism
@inproceedings{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  booktitle={Proceedings of Advances in neural information processing systems},
  pages={1223--1231},
  year={2012}
}

@Misc{NCCL,
title = {{NVIDIA collective communications library (NCCL)}},
howpublished = {\url{https://developer.nvidia.com/nccl}},
note = "[Online; accessed July-2022]",
}

@inproceedings{li2013parameter,
  title={Parameter server for distributed machine learning},
  author={Li, Mu and Zhou, Li and Yang, Zichao and Li, Aaron and Xia, Fei and Andersen, David G and Smola, Alexander},
  booktitle={Proceedings of Big Learning NIPS Workshop},
  volume={6},
  pages={2},
  year={2013}
}


@Misc{Tensorflow,
title = {{TensorFlow Homepage}},
howpublished = {\url{https://tensorflow.org/}},
note = "[Online; accessed July-2022]",
}

@Misc{MXNetWebsite,
title = {{MXNet Homepage}},
howpublished = {\url{https://mxnet.incubator.apache.org/}},
note = "[Online; accessed July-2022]",
}

@Misc{Pytorch,
title = {{Pytorch Homepage}},
howpublished = {\url{https://pytorch.org/}},
note = "[Online; accessed July-2022]",
}

@article{jia2018highly,
  title={Highly scalable deep learning training system with mixed-precision: Training imagenet in four minutes},
  author={Jia, Xianyan and Song, Shutao and He, Wei and Wang, Yangzihao and Rong, Haidong and Zhou, Feihu and Xie, Liqiang and Guo, Zhenyu and Yang, Yuanzhou and Yu, Liwei and others},
  journal={arXiv preprint arXiv:1807.11205},
  year={2018}
}


@inproceedings{luo2018parameter,
  title={Parameter hub: a rack-scale parameter server for distributed deep neural network training},
  author={Luo, Liang and Nelson, Jacob and Ceze, Luis and Phanishayee, Amar and Krishnamurthy, Arvind},
  booktitle={Proceedings of the ACM Symposium on Cloud Computing},
  pages={41--54},
  year={2018},
  organization={ACM}
}


@Misc{grpc,
title = {{gRPC - A High-Performance, Open-Source Universal RPC
Framework}},
howpublished = {\url{https://www.grpc.io/}},
note = "[Online; accessed July-2022]",
}


@Misc{pipedreamcode,
title = {{The Code Repo for PipeDream: Pipeline Parallelism for DNN Training}},
howpublished = {\url{https://github.com/msr-fiddle/pipedream}},
note = "[Online; accessed July-2022]",
}



@Misc{thrust,
title = {{The API reference guide for Thrust, the CUDA C++ template library.}},
howpublished = {\url{https://docs.nvidia.com/cuda/thrust/index.html}},
note = "[Online; accessed July-2022]",
}

@Misc{horovoddgc,
title = {{Deep gradient compression implementation in the common layer using CUDA}},
howpublished = {\url{https://github.com/horovod/horovod/pull/453
}},
note = "[Online; accessed July-2022]",
}



@Misc{NVIDIAV100,
title = {{NVIDIA TESLA V100 The First Tensor Core GPU}},
howpublished = {\url{https://www.nvidia.com/en-gb/data-center/tesla-v100/}},
note = "[Online; accessed July-2022]",
}

@inproceedings{Narayanan2019PipeDream,
author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
title = {PipeDream: Generalized Pipeline Parallelism for DNN Training},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359646},
doi = {10.1145/3341301.3359646},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {1–15},
numpages = {15},
location = {Huntsville, Ontario, Canada},
series = {SOSP ’19}
}

@inproceedings{Wang2018Tofu,
author = {Wang, Minjie and Huang, Chien-chin and Li, Jinyang},
title = {Supporting Very Large Models Using Automatic Dataflow Graph Partitioning},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303953},
doi = {10.1145/3302424.3303953},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {26},
numpages = {17},
location = {Dresden, Germany},
series = {EuroSys ’19}
}

@inproceedings{sun2019communication,
  title={Communication-efficient distributed learning via lazily aggregated quantized gradients},
  author={Sun, Jun and Chen, Tianyi and Giannakis, Georgios and Yang, Zaiyue},
  booktitle={Proceedings of Advances in Neural Information Processing Systems},
  pages={3365--3375},
  year={2019}
}
@article{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1802.04434},
  year={2018}
}

@inproceedings{huang2020swapadvisor,
author = {Huang, Chien-Chin and Jin, Gu and Li, Jinyang},
title = {SwapAdvisor: Pushing Deep Learning Beyond the GPU Memory Limit via Smart Swapping},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378530},
doi = {10.1145/3373376.3378530},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1341–1355},
numpages = {15},
keywords = {scheduling and resource management, gpu, deep learning systems},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}


@inproceedings{fan2021dapple,
  title={DAPPLE: A pipelined data parallel approach for training large models},
  author={Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={431--445},
  year={2021}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on GPU clusters using megatron-LM},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{kim2020torchgpipe,
  title={torchgpipe: On-the-fly pipeline parallelism for training giant models},
  author={Kim, Chiheon and Lee, Heungsub and Jeong, Myungryong and Baek, Woonhyuk and Yoon, Boogeon and Kim, Ildoo and Lim, Sungbin and Kim, Sungwoong},
  journal={arXiv preprint arXiv:2004.09910},
  year={2020}
}

@article{micikevicius2017mixed,
  title={Mixed precision training},
  author={Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  journal={arXiv preprint arXiv:1710.03740},
  url={https://arxiv.org/abs/1710.03740},
  year={2017}
}



@Misc{SQuAD,
title = {{Stanford Question Answering Dataset v1.1}},
howpublished = {\url{https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/}}
}


@Misc{Imagenet,
title = {{Imagenet 2012 Dataset}},
howpublished = {\url{https://www.image-net.org/}}
}


@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{wang2019supporting,
  title={Supporting very large models using automatic dataflow graph partitioning},
  author={Wang, Minjie and Huang, Chien-chin and Li, Jinyang},
  booktitle={Proceedings of the Fourteenth EuroSys Conference 2019},
  pages={1--17},
  year={2019}
}

@article{shazeer2018mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{gao2020estimating,
  title={Estimating gpu memory consumption of deep learning models},
  author={Gao, Yanjie and Liu, Yu and Zhang, Hongyu and Li, Zhengxian and Zhu, Yonghao and Lin, Haoxiang and Yang, Mao},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1342--1352},
  year={2020}
}

@Misc{bertgithub,
title = {{Bert from Google Research}},
howpublished = {\url{https://github.com/google-research/bert}}
}


@Misc{dgx-1-whitepaper,
title = {{White Paper for DGX-1 with Tesla V100}},
howpublished = {\url{https://images.nvidia.com/content/pdf/dgx1-v100-system-architecture-whitepaper.pdf}}
}

@Misc{awsp324xlarge,
title = {{Introducing to P3.24xlarge of AWS}},
howpublished = {\url{https://aws.amazon.com/about-aws/whats-new/2019/10/introducing-amazon-sagemaker-mlp3dn24xlarge-instances/}}
}

@Misc{mpressopensource,
title = {{Code of MPress}},
howpublished = {\url{https://gitlab.com/MPressX/mpress}}
}



@misc{Zheng2022Alpa,
      title={Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning}, 
      author={Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Joseph E. Gonzalez and Ion Stoica},
      year={2022},
      eprint={2201.12023},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{megatron-lm,
  author    = {Mohammad Shoeybi and
               Mostofa Patwary and
               Raul Puri and
               Patrick LeGresley and
               Jared Casper and
               Bryan Catanzaro},
  title     = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
               Model Parallelism},
  journal   = {CoRR},
  volume    = {abs/1909.08053},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.08053},
  eprinttype = {arXiv},
  eprint    = {1909.08053},
  timestamp = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-08053.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{zero-infinity,
author = {Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
title = {ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476205},
doi = {10.1145/3458817.3476205},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {59},
numpages = {14},
location = {St. Louis, Missouri},
series = {SC '21}
}

@Misc{NVSHMEM,
title = {{NVSHMEM Homepage}},
howpublished = {\url{https://developer.nvidia.com/nvshmem}},
}

@Misc{DeepSpeedCode,
title = {{DeepSpeed Code Repository}},
howpublished = {\url{https://github.com/microsoft/DeepSpeed}},
}



@inproceedings{chen2021cswap,
  title={CSWAP: A Self-Tuning Compression Framework for Accelerating Tensor Swapping in GPUs},
  author={Chen, Ping and He, Shuibing and Zhang, Xuechen and Chen, Shuaiben and Hong, Peiyi and Yin, Yanlong and Sun, Xian-He and Chen, Gang},
  booktitle={2021 IEEE International Conference on Cluster Computing (CLUSTER)},
  pages={271--282},
  year={2021},
  organization={IEEE}
}


@article{han2021pre,
  title={Pre-trained models: Past, present and future},
  author={Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and others},
  journal={AI Open},
  volume={2},
  pages={225--250},
  year={2021},
  publisher={Elsevier}
}
