\section{List}
\subsection{Parallelism}


ZeRO~\cite{zero}

ZeRO-Offload~\cite{zero-offload}

ZeRO-Infinity~\cite{zero-infinity}\

Megatron-lm~\cite{megatron-lm}

GPipe~\cite{gpipe,kim2020torchgpipe}

PipeDream~\cite{Narayanan2019PipeDream}

Tofu~\cite{Wang2018Tofu}

DAPPLE~\cite{fan2021dapple}

Supporting very large models using automatic dataflow graph partitioning~\cite{wang2019supporting}

Mesh-tensorflow~\cite{shazeer2018mesh}

Alpa~\cite{Zheng2022Alpa}

Sequence Parallelism~\cite{li2021sequence}


\subsection{Memory related}
GShard~\cite{lepikhin2020gshard}

Mixed Precision Training~\cite{micikevicius2017mixed}

Training deep nets with sublinear memory cost (Recomputation)~\cite{chen2016training}

Efficient large-scale language model training on GPU clusters using megatron-LM~\cite{narayanan2021efficient}

Reducing Activation Recomputation in Large Transformer Models~\cite{megatron3}

Checkmate: Co-design recomp + Swap~\cite{checkmate}

DELTA:\ Dynamically Optimizing GPU Memory beyond Tensor Recomputation~\cite{tang2022delta}

vDN~\cite{rhu2016vdnn}

Superneurons~\cite{wang2018superneurons}

SwapAdvisor~\cite{huang2020swapadvisor}

Estimating gpu memory consumption of deep learning models~\cite{gao2020estimating}

CSWAP:\ A Self-Tuning Compression Framework for Accelerating Tensor Swapping in GPUs~\cite{chen2021cswap}

Pre-trained models: Past, present and future~\cite{han2021pre}

Sagemaker~\cite{karakus2021amazon} and GSPMD~\cite{xu2021gspmd}propose memory efficient versions of tensor parallelism which splits activations across the devices along the hidden dimension throughout the network.





\subsection{Compression}
BinaryConnect~\cite{courbariaux2015binaryconnect}

Parallax~\cite{kim2019parallax}

Sparse communication for distributed gradient descent (Gradient Drop)~\cite{aji2017sparse}

Deep gradient compression (DGC)~\cite{lin2017deep}

Scalable distributed DNN training using commodity GPU cloud computing (MXNet-TBQ)~\cite{strom2015scalable}

Terngrad~\cite{wen2017terngrad}

Error compensated quantized SGD and its applications to large-scale distributed optimization (ECQ-SGC)~\cite{wu2018error}

Communication-efficient distributed learning via lazily aggregated quantized gradients~\cite{sun2019communication}

signSGD~\cite{bernstein2018signsgd}

\subsection{Schedule}

ByteScheduler~\cite{peng2019bytescheduler}

TicTac: Accelerating distributed deep learning with communication scheduling~\cite{hashemi2018tictac}

Priority-based parameter propagation for distributed DNN training~\cite{jayarajan2019priority}

Horovod~\cite{horovod}

Parameter Server~\cite{li2013parameter}

Parameter hub~\cite{luo2018parameter}

Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters~\cite{Peng2018Optimus}

\subsection{Items}

MXNet~\cite{chen2015mxnet, MXNetWebsite}
TensorFlow~\cite{Tensorflow}
PyTorch~\cite{Pytorch}
DeepSpeed~\cite{DeepSpeedCode,rasley2020deepspeed}
PaddlePaddle~\cite{paddlepaddlecode}
TernGrad code in PyTorch~\cite{open-source-Terngrad}
TBQ code in MXNet~\cite{open-source-TBQ}
Bank Conflict~\cite{bank-conflicts-in-GPU}
Bert~\cite{devlin2018bert}
Data Parallelism~\cite{dean2012large}
NCCL~\cite{NCCL}
gRPC~\cite{grpc}
PipeDream code~\cite{pipedreamcode}
thrust code~\cite{thrust}
DGC code in Horovod~\cite{horovoddgc}
NVIDIA V100~\cite{NVIDIAV100}
SQuAD~\cite{SQuAD}
Imagenet~\cite{Imagenet}
Bert code~\cite{bertgithub}
DGX-1~\cite{dgx-1-whitepaper}
AWS p3.24xlarge~\cite{awsp324xlarge}
MPress code~\cite{mpressopensource}
NVSHMEM~\cite{NVSHMEM}