\section{List}
\subsection{Parallelism}


ZeRO~\cite{zero}

ZeRO-Offload~\cite{zero-offload}

ZeRO-Infinity~\cite{zero-infinity}\

Megatron-lm~\cite{megatron-lm}

GPipe~\cite{gpipe}


\subsection{Memory Optimization}

Training deep nets with sublinear memory cost (Recomputation) ~\cite{chen2016training}

Reducing Activation Recomputation in Large Transformer Models~\cite{megatron3}

Checkmate: Co-design recomp + Swap ~\cite{checkmate}

DELTA: Dynamically Optimizing GPU Memory beyond Tensor Recomputation ~\cite{tang2022delta}

vDN ~\cite{rhu2016vdnn}

Superneurons ~\cite{wang2018superneurons}

\subsection{Compression}
BinaryConnect ~\cite{courbariaux2015binaryconnect}

Parallax ~\cite{kim2019parallax}

Sparse communication for distributed gradient descent (Gradient Drop) ~\cite{aji2017sparse}

Deep gradient compression (DGC) ~\cite{lin2017deep}

Scalable distributed DNN training using commodity GPU cloud computing (MXNet-TBQ) ~\cite{strom2015scalable}

Terngrad ~\cite{wen2017terngrad}

Error compensated quantized SGD and its applications to large-scale distributed optimization (ECQ-SGC) ~\cite{wu2018error}


\subsection{Schedule}

ByteScheduler ~\cite{peng2019bytescheduler}

TicTac: Accelerating distributed deep learning with communication scheduling ~\cite{hashemi2018tictac}

Priority-based parameter propagation for distributed DNN training ~\cite{jayarajan2019priority}

Horovod ~\cite{horovod}

Parameter Server ~\cite{li2013parameter}

Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters ~\cite{Peng2018Optimus}

\subsection{Items}

MXNet~\cite{chen2015mxnet, MXNetWebsite}
TensorFlow~\cite{Tensorflow}
PyTorch~\cite{Pytorch}
PaddlePaddle ~\cite{paddlepaddlecode}
TernGrad code in PyTorch ~\cite{open-source-Terngrad}
TBQ code in MXNet ~\cite{open-source-TBQ}
Bank Conflict ~\cite{bank-conflicts-in-GPU}
Bert ~\cite{devlin2018bert}
Data Parallelism ~\cite{dean2012large}
NCCL ~\cite{NCCL}



\label{sect:introv1}